对于关系型数据库而言，受伤害比较大的操作就是频繁的查询操作
在关系型数据库里面，如果频繁查询的数据但是这些数据比较固定不经常更新
就可以把这些数据摘出来
放到缓存里面，减轻数据库被访问的压力和负担

对于关系型数据库而言，事务的四大特性acid

nosql一般能做哪些事情
1、易扩展
nosql数据库种类繁多，但是一个共同的特点都是去掉关系型数据库的关系型特性
数据之间无关系，这样就非常容易扩展，也就在无形之间，在架构的层面上带来了可扩展的能力
2、大数据量高性能（一般地，redis一秒钟写可以8w次，读可以11w次，不管咋说，redis的读写能力非常强悍）
nosql数据库都具有非常高的读写性能，尤其在大数据量下，同样表现优秀
这得益于它的无关系性，数据库的结构简单
一般地，mysql使用查询缓存，每次表的更新缓存就失效，是一种大粒度的缓存
针对交互频繁的web2.0的应用中，mysql的查询缓存效率不高，而nosql的缓存是记录级别的，是小粒度的缓存
是一种细粒度的缓存，所以nosql在这个层面上来说就要性能高很多了
3、多样灵活的数据模型
nosql无需事先为要存储的数据建立字段，随时可以存储自定义的数据格式，而在关系型数据库里
增删字段是一件非常麻烦的事情，如果是非常大数据量的表，增加字段简直就是一个噩梦

比较传统的rdbms和nosql
rdbms
- 存储高度组织化结构化的数据
- 结构化查询语言（sql）
- 数据的结构和真正的数据都存储在单独的表中（表的结构，表的数据）
- 数据操纵语言，数据定义语言
- 严格的一致性（数据的强一致性）
- 基础事务
nosql
- 代表着不仅仅是sql
- 没有声明性查询语言
- 没有预定义的数据的结构
- 键值对的方式存储数据，列存储，文档存储，图形数据库等
- 最终一致性（数据的弱一致性），而非acid属性
- 非结构化的数据和不可预知的数据
- nosql数据库的CAP+BASE
- 高性能、高可用性、可伸缩性

如果你就想干一件事，专精于一件事，高速缓存的，就用memcached
如果你想expert in one and good at many things，就用redis

怎么玩redis？就这三个东西，键值对、cache、persistence

互联网时代的3v+3高
3v指的是大数据时代下的海量（Volume）、多样（Variety）、实时（Velocity）
说明：这实时一般做不到绝对的实时，都是准实时的
3高指的是高并发、高性能（高可用）、高可扩
说明：高可扩指的是水平扩展，直接加机器，而不是垂直扩展，垂直扩展说白了就是强化机器的配置

当下的应用程序一般是搭配sql和nosql一起使用滴，各有用途

能在github上面向著名开源项目发pr，别人还通过了，说明你的技术功底非常强

多数据源（你所看到的文字、图片、音频、vcr等不可能统统放到mysql里面搞定，这些数据在不同的数据源里面）和多数据类型（文字、图片、音频、vcr等）的存储问题

技术的路上，找对方向和方法，坚持就会牛p，其它事情也一样

淘宝的商品波动性，热点高频词汇（比如情人节，rose就是高频词汇）就会被放到nosql里面（内存数据库、缓存数据库）

总结大型互联网应用（大数据量、高并发、多数据源和多数据类型）的难点和解决方案
借鉴jdbc的思想（面向接口编程），诞生了统一数据平台服务层UDSL，UDSL一统天下，只需要面向UDSL编程即可

分布式微服务架构的系统非常忌讳连接查询，高并发的操作不太建议有连接查询
适当的采取冗余字段的策略来避免关联查询，比如在商品表里面冗余商品类别名称这种不太会被经常改变的字段
但是要注意改变了商品类别名称要同步更新你的那个商品表，否则就是脏数据了
分布式事务是支持不了太多的并发的

聚合的数据模型常见的有：键值对模型、bson模型、列族模型、图形模型（这四种数据模型其实对应着nosql数据库的四大分类）
说明：列族模型说白了就是纵表，我们一般都是设置横表，详情谷歌搜索横表和纵表以及应用场景，图形模型典型例子就是复杂的人际关系

学习的相通和传递性，还是那句话，多学原理、思想、方法、本质不要去学表面现象

没有那么多新的技术，切记新从旧始，思想相通，你抄袭我的，我抄袭你的，变个皮让表面现象不一样，本质是一样的

CAP定理
在理论计算机科学中，CAP定理，又被称作布鲁尔定理
它指出对于一个分布式计算系统来说
不可能同时满足以下三点
强一致性、高可用性、分区容错性
根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项，即CAP的3进2
理解CAP理论的最简单方式是想象两个节点分处分区两侧
允许至少一个节点更新状态会导致数据不一致，即丧失了C性质

提高系统的并发能力的一种常见策略，集群嘛，通过负载均衡机制提高系统并发能力，原来系统并发的能处理N1个请求，现在系统并发的能处理N2个请求（N2>>N1）

分布式和集群简介
分布式系统（distributed system）是指由多台计算机和通信的软件组件通过计算机网络连接（本地网络或广域网）组成
分布式系统是建立在网络之上的软件系统，正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性
因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件
分布式系统可以应用在在不同的平台上如：PC、工作站、局域网和广域网上等
通俗的理解分布式即不同的多台服务器上面部署不同的服务模块
他们之间通过RMI/RPC之间通信和调用
对外提供服务和组内协作
集群（人多力量大的一种感觉）指的是不同的多台服务器上面部署相同的服务模块，通过分布式调度软件进行统一的调度，对外提供服务和访问

系统架构一般分为传统的单一架构和现代化的微服务架

分布式系统的CAP定理
分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的
分布式系统的最大难点，就是各个节点的状态如何同步
CAP定理是这方面的基本定理，也是理解分布式系统的起点
分布式系统的三大指标
- Consistency: 强一致性指的是所有节点都能在同一时间返回同一份最新的数据副本
- Availability: 高可用性指的是每次请求都能够返回非错误的响应（说白了数据暂时不一致不要紧，你先给我能用，应用别蹦了）
- Partition tolerance: 分区容错性指的是服务器间的通信即使在一定时间内无法保持畅通也不会影响系统继续运行
说明：分布式系统的分区容错性通俗的理解，允许你出错，出啥错，就是分布式系统里面的两个服务器节点可能由于一些原因，导致它们之间没办法通信，这是很正常的问题，百分百会出现的问题，因此分布式系统必须具备分区容错性这一特性
参照资料
https://juejin.im/post/5da44bcbe51d4578440fe63a
在高并发的情况下，正常的哈，非常难实现（为什么难，看上面的帖子你就明白了）数据的强一致性，通俗的理解就是说，表示同一个事物的数据在分布式系统中的两个数据库节点中不一样
在高并发之后，然后再去实现数据的强一致性即可，说白了就是分布式系统中数据的最终一致性（可以理解为数据的弱一致性）
一定要理解，为什么数据的强一致性和高可用性不能同时满足
如果你想实现数据的强一致性，就得同步各个数据库集群节点的数据
数据库集群里面的那个同步操作是要锁住被同步节点的读写操作的，此时此刻你的那个被同步节点就不可用了
参照资料
https://juejin.im/post/5e8fce7f6fb9a03c585bfd1f

互联网里面99.999999999999%的应用其实都不是绝对的实时，而是准实时
比如说某人发了一条新浪微博，你一直刷新微博，眼睛一直盯着微博，然后你看到了这个人发的微博
但这并不意味着你看到这条微博的时刻就是别人微博发送成功这条微博的那个时刻，往往会间隔几秒乃至十几秒
这就是准实时，底层的原理不就是
数据的最终一致性（数据的弱一致性）+A+P，其实绝大多数互联网应用也是妥协到这种策略

nosql数据库的CAP+BASE
CAP：C表示数据的强一致性（Consistency）、A表示高可用性（Availability）、P表示分区容错性（Partition Tolerance）
BASE：BA表示基本可用（Basically Available）、S表示软状态（Soft State）、数据的最终一致性（Eventually consistent）

redis是单进程的，redis默认有16个数据库，redis默认端口号是6379
详情参照redis杂项基础知识.jpg

redis数据类型（redis将memcached干掉了的一个重要原因也是因为redis支持的数据类型更加丰富）
1、String（字符串）
2、Hash（类似于java中的Map）
3、List（列表）
4、Set（集合）
5、Zset（sorted set：有序集合）
说明：Zset只是在Set的基础上加了一个score值，Set是sadd k1 v1 v2，Zset是zadd k1 score1 v1 score2 v2

哪里去获取redis数据类型的命令操作，参照官方文档，当然不仅限官方文档，菜鸟教程、B站等各种地方也找的到学习资源，找资源也是一种能力，多摸索

核心知识，配置redis源码安装后的配置文件redis.conf

redis缓存达到设置的maxMemory后，移除缓存的策略，要根据你具体的项目的业务场景来灵活配置，很多东西的策略都要根据具体的项目的业务场景来定策略

redis持久化，不管是RDB还是AOF，泛泛的理解，就是给内存里面的数据保存到文件里面去了
redis持久化之RDB（redis database）
在指定的时间间隔内将内存中的数据集的快照写入到磁盘（就是那个dump.rdb文件），行话讲的就是Snapshot快照，它恢复时是将快照文件直接读到内存里面
redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中
等待持久化过程都结束了，再利用这个临时文件替换掉上次持久化好了的文件（泛泛的理解就是替换掉旧的dump.rdb文件）
说明：你要保证你持久化的数据是最新的那个时间轮换区，这样才是有意义的
上面自己造的一个概念，时间轮换区，现在redis干活，随着时间推移数据会往缓存里面存放
假设第一个五分钟内redis里面可能只有5个键值对，第二个五分钟内redis里面可能就有20个键值对了
这里面的第一个五分钟内，第二个五分钟内就是时间轮换区
在整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能
如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感（对数据精度要求不是非常之高），可以考虑采用RDB方式实现redis持久化
说明：这种大规模数据的恢复存在一定的问题，你期望的是你从源表往redis里面灌数据的时候的同时你的源表是静态的，这样才不会出现数据不一致的情况
但是实际上并不是这样，你基于源表往redis里面放数据的时候，你的源表又不是锁定状态，源表的数据在此期间如果有变化也是正常的
那RDB方式要比AOF方式更加的高效一些，RDB的缺点就是最后一次持久化后的数据可能会丢失
说明：理解最后一次持久化后的数据可能会丢失，注意是可能
假设比较极端的情况你刚好要将某一次时间轮换区的数据更新到dump.rdb文件里面去的时刻
这个时刻机器出故障了，这种情况下你是不是丢失掉了这一次时间轮换区内更新的数据啊！！！
下次拿dump.rdb文件来恢复数据到内存里面去，内存里面的数据其实是不准确滴！！！
说明：fork的作用是复制一个与当前进程一样的进程，新进程的所有数据（变量、环境变量、程序计数器等）数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程
RDB方式持久化默认其实生成了的是dump.rdb文件
redis持久化之RDB方式其实就是通过偷偷的在后台帮你备份内存的数据到dump.rdb文件来实现的，几分钟备份一次内存的数据可以在配置文件里面设置，通俗的理解就是我几分钟更新一下这个dump.rdb文件
因此，这个dump.rdb文件非常重要哈，需要做容灾备份，写个定时任务啥之类的将这个物理文件定时的备份到另外一台机器上面去
千万注意：特殊情况1，FLUSH啥啥啥命令是给redis内存里面的数据清空并立马生成新的dump.rdb文件
也就是说不需要等待那个配置几分钟备份一次的时间我立马就给你这个dump.rdb文件变成最新的，而且里面的内容是空，你拿这个文件来恢复数据到内存里面去，内存里面肯定也没有数据了
特殊情况2，使用save命令手动备份，表示我要立马把redis缓存里面的数据备份到dump.rdb文件里面
如何通过dump.rdb这个备份文件来恢复数据到内存里面去，只需要把dump.rdb文件放到某一个目录下，然后在该目录下启动redis服务即可，并且在该目录下使用redis客户端进程（redis-cli）进行相关操作
在centos上面成功编译安装了redis后是可以在任何目录结构下使用redis相关指令，（原理应该是编译安装过程中给你配置了类似于Windows系统里面的那个path环境变量），比如redis-server、redis-cli等指令
如何关闭redis进程，使用redis-cli指令连接上redis服务端后，使用shutdown指令，还有就是kill，不多解释了，说的太详细了(*￣︶￣)
redis持久化之RDB方式适合大规模的数据恢复，对数据的完整性和一致性要求不高

redis持久化之AOF（append only file，AOF的出现其实也是为了弥补RDB的不足）
其实也没必要解决RDB残留的问题，因为丢个15分钟内的数据，运维工程师分分钟能解决，但是还是那句话，追求完美！！！
AOF通俗的理解就是redis每次写操作都默认会被记录到appendonly.aof文件里面
如果因为网络、断电了等各种原因，导致我写了一半还没将所有写操作都记录到aof文件里面，导致aof文件挂掉了，即aof文件损坏了
那么这个时候redis服务还起的起来吗？测试结果，redis服务起不来
结论：dump.rdb文件和appendonly.aof文件可以同时存在，但是默认找的是appendonly.aof文件
那么怎么样修复已经损坏的aof文件呢？使用redis-check-aof指令（后面带上一些指令的参数，具体可以看官方文档，不要去记命令，而要理解基本原理）
aof采用追加记录的方式来工作，这样势必会导致aof文件越来越大，咋办呢？
对着上一个疑问，引入了aof重写机制
1、是啥，aof采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制
当aof文件的大小超过所设定的阈值时，redis就会启动aof文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof
2、aof重写机制的原理，aof文件持续增长而过大时，会fork出一条新进程来将文件重写（先写临时文件最后再rename）
遍历新进程的内存中数据，每条记录有一条的Set语句,重写aof文件的操作，并没有读取旧的aof文件
而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似
3、什么时候触发aof机制，redis会记录上次重写时的aof文件大小
默认配置是当aof文件大小是上次rewrite后大小的一倍且文件大于64M时触发

redis事务是什么
redis事务指的是可以一次执行多个redis命令，本质是一组redis命令的集合
一个redis事务中的所有redis命令都会被序列化，按顺序的串行的执行而不会被其它命令插入，不许加塞
加塞说白了就是这个数据本身我当前事务来修改的，但是同时另一个事务也并发的过来凑热闹，默认情况下不作任何处理，这就可能会出现数据安全的问题

redis事务能干啥
它可以在一个队列里面，能够一次性、有序的、排它的执行一系列redis命令

redis事务几种情况
正常执行、放弃事务（discard）、全体连坐（一个redis命令出错了，一个redis事务里面所有的redis命令都算失败）、冤头债主（redis命令对的放行，错的挂掉）、watch监控（详情参照redis事务之watch监控.jpg、思维导图）
因此，redis是部分支持事务，不像Oracle、MySQL、SQL Server等关系型数据库的事务那样，必须是acid四大特性

多个数据库事务并发操作同一记录引起的问题主要有两大方向，一是读问题，三种，分别是脏读，不可重复读，幻读，二是写问题，一种，丢失更新

非常重要！！！
以下说明针对关系型数据库哈哈哈
悲观锁（用的很少）
往往在高并发的情况下
认为一定有非己的数据库连接（非己的事务）会改当前的数据库连接（当前的事务）想要改的数据，那么采取锁的措施，能保证数据不出问题但是性能低，很少使用
可以在“夜深人静”的时候，访问数据库较少的时间段内可以使用，备份数据库之前加上悲观锁来保证数据不会有问题，备份的时间期间内其它事务无法操作数据
乐观锁（这个才是需要掌握工作中用的多）
往往在高并发的情况下
用“版本号”的机制来做判断这个事务到底应该被提交还是被回滚，一般用于解决“丢失更新”的问题
总而言之，数据库里面的各种锁其实就是为了保证多个数据库事务并发操作同一记录的场景下，数据仍然不出现问题

使用乐观锁解决“丢失更新”的问题的原理
造成“丢失更新”的问题的根本原因在于不同的数据库事务并发的写操作同一记录的时候
由于并发问题（时间间隔非常短）导致它们修改记录之前读取到的要被修改的数据是一样的
因此它们各自内部执行修改数据的时候参照的就是它们各自之前读取到的数据
这整个过程中，假设有的事务对原始数据做了修改并且该事务成功提交了
默认情况下，其它事务是不知道上面说的那个事务改了原始数据并成功提交了的
因此它们还是参照原始数据进行修改然后提交事务或者回滚事务，这样就产生丢失更新问题，有点类似svn、git过期的revision，造成conflict
期望实现的效果，在这个并发模型当中
每一个事务进行数据的更新操作的时候，一定要确保是基于最新的数据进行更新，否则这个事务不能成功提交
因此，出现了版本号这个策略，一般用于解决“丢失更新”的问题
版本号这个策略，在数据库中要被并发修改的数据的所在表里面加一个version字段
在并发事务集合中，任意一个事务在提交事务之前的一个环节
就是需要比对一下当前事务最开始读取到version是否和现在这个时刻数据库表里面将被修改的记录的version是否一样
是一样的事务提交成功，否则事务提交失败，这样解决了丢失更新问题

单条的查询语句也是一个执行单元，因此也是可以组成独立的一个事务

两个不同的数据库连接会话/数据库连接对象各自对应的JDBC操作一定处于不同的数据库事务当中
两个一样的数据库连接会话/数据库连接对象所包含的所有JDBC操作可以不处于同一个数据库事务当中

核心补充，比如现在有N个JDBC操作，业务上面要求这N个JDBC操作必须保证原子性才认为是合乎情理的
言外之意就是这N个JDBC操作必须在同一个数据库事务当中
要想保证这N个JDBC操作必须在同一个数据库事务当中，它的充分条件是这N个JDBC操作必须使用一样的数据库连接会话/数据库连接对象

通俗解释总结脏读、不可重复读、幻读问题
脏读侧重点在于未提交更新下的并发问题，幻读侧重点在于未提交插入下的并发问题，这两个问题很类似
不可重复读也是一种并发问题，在一个事务中查询多次但查询结果是不一样的，往往是由于另一个事务提交或者回滚引起的并发问题

通俗解释总结丢失更新问题
多个数据库事务并发操作同一记录，默认情况下最后提交/回滚的那个事务，会对前面提交/回滚的事务里面更改的数据造成丢失的影响
如果不好理解的话
可以参照下面的帖子看下，理解一下
https://blog.csdn.net/z69183787/article/details/52213670
https://www.jianshu.com/p/d8bc0a843dd0

那怎么样解决这种多个数据库事务并发操作同一记录引起的读问题（写问题就那个“丢失更新”的问题，一般用乐观锁就可以解决了）
事务的隔离级别（它的底层，或者说它的本质上就是通过一定的机制和锁来解决上述问题的）
不考虑事务的隔离级别，多个数据库事务并发操作同一记录引起的读问题就可能发生，因此需要根据具体的项目的业务场景去指定事务的隔离级别

补充学习总结的细节知识
MySQL数据库中事务默认是自动提交/回滚（隐式提交/回滚）的，Oracle数据库中事务默认是手动提交/回滚（显式提交/回滚）的
当然一些“特殊的情况”，数据库会给你悄咪咪的自动提交/回滚
可以手动修改数据库中事务默认提交/回滚的方式，一般就两种手动和自动
这里面的细节比较多，不同类型的数据库不一样，甚至同一个类型的数据库的不同版本间可能也有些差别，不能靠死记硬背结论，要去做实验去实践验证
标题中“特殊的情况”说明，以Oracle数据库为例
https://blog.csdn.net/tayanxunhua/article/details/9257971
学习一定要明确“纸上学来终觉浅，绝知此事要躬行”，要去做实验去实践验证，说句难听的，很多帖子都是灌水帖

redis消息的发布和订阅机制是什么
它是进程间的一种消息通信模式，发布者进程（消息的提供者进程）发送消息，订阅者进程（消息的消费者进程）接收消息
redis存在这种消息的发布和订阅机制，说白了redis可以作为消息中间件，但一般不使用redis作为消息中间件

redis主从复制有什么用，和mysql主从复制的作用类似，实现读写分离（写走主库读走从库，这样可以减轻服务器的IO负担），容灾恢复
todo 后期深入学习继续更新
